{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'In', 'an', 'Oct.', '19', 'review', 'of', '``', 'The', 'Misanthrope']\n",
      "['BEGIN_SENT', 'IN', 'DT', 'NNP', 'CD', 'NN', 'IN', '``', 'DT', 'NN']\n",
      "1029693\n",
      "1029693\n"
     ]
    }
   ],
   "source": [
    "# extract words and their POS tags from the WSJ corpus\n",
    "with open('./corpus/WSJ_02-21.pos') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "words = ['<s>']\n",
    "pos_tags = ['BEGIN_SENT']\n",
    "\n",
    "for line in data:\n",
    "    if len(line) > 1:\n",
    "        word, pos = line.split()\n",
    "        words.append(word)\n",
    "        pos_tags.append(pos)\n",
    "    else:\n",
    "        words.extend(['</s>', '<s>'])\n",
    "        pos_tags.extend(['END_SENT', 'BEGIN_SENT'])\n",
    "\n",
    "print(words[:10])\n",
    "print(pos_tags[:10])\n",
    "print(len(words))\n",
    "print(len(pos_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'an': 3142, 'The': 6795, 'the': 41098, 'a': 19264, 'A': 817, 'some': 1274, 'this': 1897, 'any': 721, 'those': 505, 'Both': 99, 'Some': 314, 'no': 606, 'An': 141, 'Either': 3, 'This': 398, 'these': 417, 'another': 351, 'that': 1168, 'That': 412, 'each': 356, 'every': 171, 'all': 842, 'No': 82, 'both': 336, 'These': 139, 'Another': 72, 'Those': 66, 'Each': 57, 'Any': 18, 'All': 82, 'THE': 35, 'either': 50, 'many': 14, 'Every': 20, 'neither': 18, 'NO': 2, 'half': 31, 'Many': 2, 'Neither': 13, 'nary': 1, 'AN': 6, 'them': 1, 'la': 2, 'Half': 1, 'THOSE': 1, 'del': 1, 'BOTH': 1})\n",
      "defaultdict(<class 'int'>, {'<s>': 39833})\n",
      "defaultdict(<class 'int'>, {'</s>': 39832})\n"
     ]
    }
   ],
   "source": [
    "# a table of frequencies of words that occur with that POS tag\n",
    "from collections import defaultdict\n",
    "\n",
    "likelihoods = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for word, pos in zip(words, pos_tags):\n",
    "    likelihoods[pos][word] += 1\n",
    "\n",
    "print(likelihoods['DT'])\n",
    "print(likelihoods['BEGIN_SENT'])\n",
    "print(likelihoods['END_SENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'IN': 5050, 'NNP': 8036, 'DT': 8648, 'NNS': 1669, 'PRP': 2428, 'JJ': 1671, '``': 3003, 'RB': 2263, 'CC': 2269, 'WP': 123, 'VBG': 482, 'VBN': 230, 'CD': 440, 'NN': 1598, 'PRP$': 320, 'WDT': 31, 'VBD': 33, '(': 144, 'VBZ': 56, 'FW': 7, 'NNPS': 79, 'WRB': 249, 'JJR': 60, 'JJS': 100, 'VB': 115, 'LS': 28, 'TO': 138, 'PDT': 29, 'RBR': 84, '$': 28, 'MD': 27, 'RBS': 22, 'EX': 168, 'VBP': 14, ':': 102, 'UH': 23, 'SYM': 46, \"''\": 16, ')': 1, 'WP$': 1, '#': 1})\n",
      "defaultdict(<class 'int'>, {'BEGIN_SENT': 39832})\n",
      "defaultdict(<class 'int'>, {'NNP': 9044, 'NN': 38873, 'JJ': 17850, 'NNPS': 416, 'VBN': 689, 'NNS': 5988, 'CD': 1876, 'WP': 66, '(': 43, 'RBR': 141, 'VBG': 628, 'JJR': 465, 'RB': 844, 'WRB': 1, 'IN': 791, 'RBS': 228, 'DT': 129, 'VBZ': 657, '``': 454, 'MD': 181, 'JJS': 762, 'TO': 25, 'VBP': 145, ',': 191, 'PRP$': 59, '.': 131, 'VBD': 180, '$': 757, 'FW': 21, 'END_SENT': 7, 'CC': 60, 'VB': 21, 'PRP': 40, 'WDT': 16, \"''\": 3, '#': 13, ':': 33, 'POS': 3, 'RP': 6, ')': 2, 'UH': 2, 'PDT': 1})\n"
     ]
    }
   ],
   "source": [
    "# a table of frequencies of following states\n",
    "transitions = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for i in range(len(pos_tags) - 1):\n",
    "    transitions[pos_tags[i]][pos_tags[i + 1]] += 1\n",
    "\n",
    "print(transitions['BEGIN_SENT'])\n",
    "print(transitions['END_SENT'])\n",
    "print(transitions['DT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'an': 0.03839104616211725, 'The': 0.08302583025830258, 'the': 0.5021627037462427, 'a': 0.2353803670487036, 'A': 0.009982649495369126, 'some': 0.015566579506854672, 'this': 0.023178807947019868, 'any': 0.008809657633000172, 'those': 0.006170425942670023, 'Both': 0.0012096478580679846, 'Some': 0.003836660883165123, 'no': 0.007404511131204028, 'An': 0.0017228317978544023, 'Either': 3.665599569902984e-05, 'This': 0.004863028762737959, 'these': 0.005095183402165147, 'another': 0.004288751496786491, 'that': 0.014271400992155616, 'That': 0.0050340900760000975, 'each': 0.0043498448229515405, 'every': 0.002089391754844701, 'all': 0.010288116126194374, 'No': 0.0010019305491068156, 'both': 0.0041054715182913416, 'These': 0.0016983944673883825, 'Another': 0.0008797438967767161, 'Those': 0.0008064319053786564, 'Each': 0.0006964639182815669, 'Any': 0.00021993597419417904, 'All': 0.0010019305491068156, 'THE': 0.00042765328315534813, 'either': 0.0006109332616504973, 'many': 0.00017106131326213923, 'Every': 0.0002443733046601989, 'neither': 0.00021993597419417904, 'NO': 2.4437330466019892e-05, 'half': 0.00037877862222330833, 'Many': 2.4437330466019892e-05, 'Neither': 0.0001588426480291293, 'nary': 1.2218665233009946e-05, 'AN': 7.331199139805968e-05, 'them': 1.2218665233009946e-05, 'la': 2.4437330466019892e-05, 'Half': 1.2218665233009946e-05, 'THOSE': 1.2218665233009946e-05, 'del': 1.2218665233009946e-05, 'BOTH': 1.2218665233009946e-05})\n",
      "defaultdict(<class 'int'>, {'IN': 0.12678248644306087, 'NNP': 0.20174733882305684, 'DT': 0.2171118698533842, 'NNS': 0.04190098413336011, 'PRP': 0.06095601526410926, 'JJ': 0.04195119501908014, '``': 0.07539164490861619, 'RB': 0.05681361719220727, 'CC': 0.05696424984936734, 'WP': 0.0030879694717814822, 'VBG': 0.012100823458525808, 'VBN': 0.005774251857802772, 'CD': 0.011046394858405303, 'NN': 0.04011849769029926, 'PRP$': 0.008033741715203857, 'WDT': 0.0007782687286603736, 'VBD': 0.0008284796143803977, '(': 0.003615183771841735, 'VBZ': 0.0014059048001606748, 'FW': 0.00017573810002008435, 'NNPS': 0.001983329985940952, 'WRB': 0.006251255272143001, 'JJR': 0.001506326571600723, 'JJS': 0.002510544286001205, 'VB': 0.002887125928901386, 'LS': 0.0007029524000803374, 'TO': 0.0034645511146816628, 'PDT': 0.0007280578429403495, 'RBR': 0.002108857200241012, '$': 0.0007029524000803374, 'MD': 0.0006778469572203253, 'RBS': 0.0005523197429202651, 'EX': 0.004217714400482024, 'VBP': 0.0003514762000401687, ':': 0.0025607551717212293, 'UH': 0.0005774251857802771, 'SYM': 0.0011548503715605543, \"''\": 0.0004016870857601928, ')': 2.510544286001205e-05, 'WP$': 2.510544286001205e-05, '#': 2.510544286001205e-05})\n"
     ]
    }
   ],
   "source": [
    "# converting to probabilities by dividing by the total\n",
    "for pos in likelihoods:\n",
    "    total = sum(likelihoods[pos].values())\n",
    "    for word in likelihoods[pos]:\n",
    "        likelihoods[pos][word] /= total\n",
    "\n",
    "for pos in transitions:\n",
    "    total = sum(transitions[pos].values())\n",
    "    for next_pos in transitions[pos]:\n",
    "        transitions[pos][next_pos] /= total\n",
    "\n",
    "print(likelihoods['DT'])\n",
    "print(transitions['BEGIN_SENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44391, 47)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabularies = set(words)\n",
    "unique_pos_tags = list(transitions.keys())\n",
    "len(vocabularies), len(unique_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>/BEGIN_SENT life/NN ./NNS </s>/VBP \n"
     ]
    }
   ],
   "source": [
    "# implement a vertebi HMM POS tagger\n",
    "sample_sentence = \"<s> life . </s>\".split()\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "# initialize the viterbi matrix\n",
    "viterbi = np.zeros((len(unique_pos_tags), len(sample_sentence)))\n",
    "\n",
    "for j, token in enumerate(sample_sentence):\n",
    "    for i, pos in enumerate(unique_pos_tags):\n",
    "        if j == 0 and pos == 'BEGIN_SENT':\n",
    "            viterbi[i, j] = 1\n",
    "        elif j == 1:\n",
    "            viterbi[i, j] = transitions['BEGIN_SENT'][pos] * likelihoods[pos][token]\n",
    "        else:\n",
    "            viterbi[i, j] = max(viterbi[k, j - 1] * transitions[unique_pos_tags[k]][pos] * likelihoods[pos][token] for k in range(len(unique_pos_tags)))\n",
    "        \n",
    "# backtrace to find the best path\n",
    "best_path = [np.argmax(viterbi[:, -1])]\n",
    "for j in range(len(sample_sentence) - 1, 0, -1):\n",
    "    best_path.append(np.argmax([viterbi[i, j - 1] * transitions[unique_pos_tags[i]][unique_pos_tags[best_path[-1]]] for i in range(len(unique_pos_tags))]))\n",
    "best_path = best_path[::-1]\n",
    "\n",
    "\n",
    "# print the best path\n",
    "for i, token in enumerate(sample_sentence):\n",
    "    print(f'{token}/{pos_tags[best_path[i]]}', end=' ')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
