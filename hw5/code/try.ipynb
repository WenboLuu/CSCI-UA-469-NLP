{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_extract(word):\n",
    "    if not word:\n",
    "        return None\n",
    "    prefixes = [\"anti\", \"dis\", \"extra\", \"inter\", \"pre\", \"re\", \"sub\", \"un\", \"in\", \"im\", \"ir\", \"il\", \"over\", \"under\", \"trans\", \"mis\", \"non\", \"co\", \"com\", \"con\", \"de\", \"auto\", \"bio\", \"geo\", \"psycho\"]\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            return prefix\n",
    "    return '@@'  \n",
    "\n",
    "def suffix_extract(word):\n",
    "    if not word:\n",
    "        return None\n",
    "    suffixes = [\"able\", \"ible\", \"ation\", \"ment\", \"ness\", \"ity\", \"ty\", \"ly\", \"ing\", \"ed\", \"ize\", \"ise\", \"ful\", \"less\", \"ous\", \"ive\", \"al\", \"er\", \"or\", \"ism\", \"ist\", \"ship\", \"hood\", \"th\", \"en\", \"ify\", \"ward\", \"wise\"]\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return suffix\n",
    "    return '@@'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/WSJ_02-21.pos-chunk'  \n",
    "\n",
    "sentences = []\n",
    "sentence = []\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():  \n",
    "            word = line.strip().split('\\t')\n",
    "            sentence.append(word)\n",
    "        else:  \n",
    "            if sentence:  \n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "\n",
    "# Don't forget to add the last sentence if the file doesn't end with a newline\n",
    "if sentence:\n",
    "    sentences.append(sentence)\n",
    "\n",
    "\n",
    "import nltk\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def is_capitalized(word):\n",
    "    return word[0].isupper()\n",
    "\n",
    "def extract_features(sentences):\n",
    "    features = []\n",
    "    for sentence in sentences:\n",
    "        for i, token in enumerate(sentence):\n",
    "            word, pos, bio_tag = token\n",
    "            feature = {\n",
    "                'WORD': word,\n",
    "                'STEM': stem(word),\n",
    "                'POS': pos,\n",
    "                'LENGTH': len(word),\n",
    "                'PREFIX': prefix_extract(word),\n",
    "                'SUFFIX': suffix_extract(word),\n",
    "                'POSITION': round(i / len(sentence), 2),\n",
    "                'CAPITALIZED': is_capitalized(word),\n",
    "                'PREVIOUS_TAG': sentence[i-1][2] if i > 0 else \"@@\",\n",
    "                'PREVIOUS_POS': sentence[i-1][1] if i > 0 else \"@@\",\n",
    "                'PREVIOUS_WORD': sentence[i-1][0] if i > 0 else \"@@\",\n",
    "                'PREVIOUS_STEM': stem(sentence[i-1][0]) if i > 0 else \"@@\",\n",
    "                'PREVIOUS_2_POS': sentence[i-2][1] if i > 1 else \"@@\",\n",
    "                'PREVIOUS_2_WORD': sentence[i-2][0] if i > 1 else \"@@\",\n",
    "                'PREVIOUS_2_STEM': stem(sentence[i-2][0]) if i > 1 else \"@@\",\n",
    "                'PREVIOUS_3_STEM': stem(sentence[i-3][0]) if i > 2 else \"@@\",\n",
    "                'NEXT_TAG': sentence[i+1][2] if i < len(sentence)-1 else \"@@\",\n",
    "                'NEXT_POS': sentence[i+1][1] if i < len(sentence)-1 else \"@@\",\n",
    "                'NEXT_WORD': sentence[i+1][0] if i < len(sentence)-1 else \"@@\",\n",
    "                'NEXT_STEM': stem(sentence[i+1][0]) if i < len(sentence)-1 else \"@@\",\n",
    "                'NEXT_2_POS': sentence[i+2][1] if i < len(sentence)-2 else \"@@\",\n",
    "                'NEXT_2_WORD': sentence[i+2][0] if i < len(sentence)-2 else \"@@\",\n",
    "                'NEXT_2_STEM': stem(sentence[i+2][0]) if i < len(sentence)-2 else \"@@\",\n",
    "            }\n",
    "            features.append((feature, bio_tag))\n",
    "        # mark for new line\n",
    "        features.append(('NEWLINE', None))\n",
    "    return features\n",
    "\n",
    "features = extract_features(sentences)\n",
    "\n",
    "with open(r'..\\bin\\a.features', 'w') as f:\n",
    "    f.write(\"\\n\")\n",
    "    for feature, bio_tag in features:\n",
    "        if feature == 'NEWLINE':\n",
    "            f.write('\\n')\n",
    "            continue\n",
    "        for key, value in feature.items():\n",
    "            if key == 'WORD':\n",
    "                f.write(f\"{feature['WORD']}\\t\")\n",
    "            else:\n",
    "                f.write(f\"{key}={value}\\t\")\n",
    "        f.write(f\"{bio_tag}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '../data/WSJ_23.pos'  \n",
    "file_path = '../data/WSJ_24.pos'\n",
    "\n",
    "sentences = []\n",
    "sentence = []\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():  \n",
    "            word = line.strip().split('\\t')\n",
    "            sentence.append(word)\n",
    "        else:  \n",
    "            if sentence:  \n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "\n",
    "# Don't forget to add the last sentence if the file doesn't end with a newline\n",
    "if sentence:\n",
    "    sentences.append(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def is_capitalized(word):\n",
    "    return word[0].isupper()\n",
    "\n",
    "def extract_features(sentences):\n",
    "    features = []\n",
    "    for sentence in sentences:\n",
    "        for i, token in enumerate(sentence):\n",
    "            word, pos = token\n",
    "            feature = {\n",
    "                'WORD': word,\n",
    "                'STEM': stem(word),\n",
    "                'POS': pos,\n",
    "                'LENGTH': len(word),\n",
    "                'PREFIX': prefix_extract(word),\n",
    "                'SUFFIX': suffix_extract(word),\n",
    "                'POSITION': round(i / len(sentence), 2),\n",
    "                'CAPITALIZED': is_capitalized(word),\n",
    "                'PREVIOUS_POS': sentence[i-1][1] if i > 0 else \"@@\",\n",
    "                'PREVIOUS_WORD': sentence[i-1][0] if i > 0 else \"@@\",\n",
    "                'PREVIOUS_STEM': stem(sentence[i-1][0]) if i > 0 else \"@@\",\n",
    "                'PREVIOUS_2_POS': sentence[i-2][1] if i > 1 else \"@@\",\n",
    "                'PREVIOUS_2_WORD': sentence[i-2][0] if i > 1 else \"@@\",\n",
    "                'PREVIOUS_2_STEM': stem(sentence[i-2][0]) if i > 1 else \"@@\",\n",
    "                'NEXT_POS': sentence[i+1][1] if i < len(sentence)-1 else \"@@\",\n",
    "                'NEXT_WORD': sentence[i+1][0] if i < len(sentence)-1 else \"@@\",\n",
    "                'NEXT_STEM': stem(sentence[i+1][0]) if i < len(sentence)-1 else \"@@\",\n",
    "                'NEXT_2_POS': sentence[i+2][1] if i < len(sentence)-2 else \"@@\",\n",
    "                'NEXT_2_WORD': sentence[i+2][0] if i < len(sentence)-2 else \"@@\",\n",
    "                'NEXT_2_STEM': stem(sentence[i+2][0]) if i < len(sentence)-2 else \"@@\",\n",
    "            }\n",
    "            features.append(feature)\n",
    "        # mark for new line\n",
    "        features.append('NEWLINE')\n",
    "    return features\n",
    "\n",
    "features = extract_features(sentences)\n",
    "\n",
    "with open(r'..\\bin\\b.features', 'w') as f:\n",
    "    f.write(\"\\n\")\n",
    "    for feature in features:\n",
    "        if feature == 'NEWLINE':\n",
    "            f.write('\\n')\n",
    "            continue\n",
    "        for key, value in feature.items():\n",
    "            if key == 'WORD':\n",
    "                f.write(f\"{feature['WORD']}\\t\")\n",
    "            else:\n",
    "                f.write(f\"{key}={value}\\t\")\n",
    "        f.write(\"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
